{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Keras & TensorFlow - The Keras workflow, expanded \n",
    "## TensorFlow Dev Summit 2017\n",
    "### YouTube ID: UeheTiBJ0Io\n",
    "### My source code: https://github.com/hcchengithub/Toy-Video-QA-problem-on-Keras\n",
    "\n",
    "The code shown on the YouTube video is very not working. I have fixed all problems and now . . . don't know how to play yet.\n",
    "This command line:\n",
    "\n",
    "    jupyter nbconvert \"experimenting the source code.ipynb\"  --to script\n",
    "\n",
    "converts this jupyter notebook into a .py normal python script file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = tf.keras.layers.Input(shape=(None,150,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.applications.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix a tf 1.4 bug before going on\n",
    "If your tf version is 1.4 then you probably have the same problem that will happen to the next cell:\n",
    "\n",
    "    %USERPROFILE%\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\wrappers.py in _compute_output_shape(self, input_shape)\n",
    "        191                                                  input_shape[2:])\n",
    "        192     child_output_shape = self.layer._compute_output_shape(  # pylint: disable=protected-access\n",
    "    --> 193         child_input_shape).as_list()\n",
    "        194     timesteps = input_shape[1]\n",
    "        195     return tensor_shape.TensorShape([child_output_shape[0], timesteps] +\n",
    "\n",
    "    AttributeError: 'NoneType' object has no attribute 'as_list'\n",
    "\n",
    "[Tensorflow issue#14054](https://github.com/tensorflow/tensorflow/issues/14054) has fixed the problem. You need to manually modify `%USERPROFILE%\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\site-packages\\tensorflow\\python\\layers\\base.py` a little to fix the bug. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_frames = tf.keras.layers.TimeDistributed(cnn)(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_vid = tf.keras.layers.LSTM(256)(encoded_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = tf.keras.layers.Input(shape=[100],dtype='int32')  # not (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Embedding(10000,256,mask_zero=True)(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_q = tf.keras.layers.LSTM(128)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.concatenate([encoded_vid,encoded_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dense(128,activation=tf.nn.relu)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.keras.layers.Dense(1000)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model([video,question],outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_absolute_percentage_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model` compiled ok then what? A hint from YouTube comment says: They put a version of this model on the Keras docs. No sample data though (as below) Actually https://github.com/keras-team/keras > somewhere \n",
    "\n",
    "### [Video question answering model](https://keras.io/getting-started/functional-api-guide/#visual-question-answering-model)\n",
    "\n",
    "Now that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" -> \"football\").\n",
    "    \n",
    "    from keras.layers import TimeDistributed\n",
    "    video_input = Input(shape=(100, 224, 224, 3))\n",
    "    # This is our video encoded via the previously trained vision_model (weights are reused)\n",
    "    encoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # the output will be a sequence of vectors\n",
    "    encoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n",
    "\n",
    "    # This is a model-level representation of the question encoder, reusing the same weights as before:\n",
    "    question_encoder = Model(inputs=question_input, outputs=encoded_question)\n",
    "\n",
    "    # Let's use it to encode the question:\n",
    "    video_question_input = Input(shape=(100,), dtype='int32')\n",
    "    encoded_video_question = question_encoder(video_question_input)\n",
    "\n",
    "    # And this is our video question answering model:\n",
    "    merged = keras.layers.concatenate([encoded_video, encoded_video_question])\n",
    "    output = Dense(1000, activation='softmax')(merged)\n",
    "    video_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
